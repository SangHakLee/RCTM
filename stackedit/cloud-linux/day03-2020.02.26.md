day02 이어서

# 스토리지 구성 및 관리

## DAS

### swap
```bash
$ swapon -s
```
#### swap 만들기
```bash
$ lsblk -f
NAME     FSTYPE      LABEL     UUID                                   MOUNTPOINT
sda                                                                   
├─sda1   xfs         red_disk  eb7da13f-0cb3-4733-a3bc-56201934f563   /red
└─sda2                                                                

$ mkswap /dev/sda2
Setting up swapspace version 1, size = 204796 KiB
no label, UUID=68f82a7d-dbdf-4da0-bdac-7b9679714f5d

$ lsblk -f
NAME     FSTYPE      LABEL     UUID                                   MOUNTPOINT
sda                                                                   
├─sda1   xfs         red_disk  eb7da13f-0cb3-4733-a3bc-56201934f563   /red
└─sda2   swap                  68f82a7d-dbdf-4da0-bdac-7b9679714f5d   

$ swapon -s
Filename				Type		Size	Used	Priority
/dev/dm-1                              	partition	3145724	0	-1

$ swapon /dev/sda2 # swap 활성화

$ swapon -s
Filename				Type		Size	Used	Priority
/dev/dm-1                              	partition	3145724	0	-1
/dev/sda2                              	partition	204796	0	-2

$ swapoff /dev/sda2 # 우선 순위 지정을 위해 비활성화

$ vi /etc/fstab # 
	/dev/sda2               swap                    swap    defaults,pri=3  0 0

$ mount -a 
$ swapon -s # mount -a로 swap이 안되는 것을 확인
Filename				Type		Size	Used	Priority
/dev/dm-1                              	partition	3145724	0	-1

$ swapon -a
$ swapon -s
Filename				Type		Size	Used	Priority
/dev/dm-1                              	partition	3145724	0	-1
/dev/sda2                              	partition	204796	0	3

```
- Priority: 양의 정수에 가까울수록 우선 순위가 높음

### LVM

1TB  볼륨 100개를 합쳐 100TB의 볼륨 그룹을 만든다.

- Physical Volumes: 1TB의 물리적인 볼륨
- Volume Groups: 물리 볼륨 여러개를 모아 둔 것
- Logical Volume: 볼륨 그룹을 논리적으로 나눈것
- Extend: 용량,가 관리의 최소 단위, 볼륨 그룹을 만들 때 결정됨(default: 4MB)
	- 7MB를 원하면 extend 2개를 사용하면 됨

#### LVM 실습
1. vm 강제 종료 `Force Off`(VMM)
2. 이전에 찍어둔 스냅샷으로 롤백 (컴퓨터 2개 합친 아이콘)
3. 디스크 3개 추가 (이전과 동일,  Virtual Machine Manager)
4. LVM 실습, bash

```bash
$ pvs # 물리 볼륨 확인, LVM 전이라 추가한 3개 볼륨이 보이지 않음음
  PV         VG Fmt  Attr PSize  PFree
  /dev/vda2  cl lvm2 a--  29.00g    0

$ pvcreate /dev/sda /dev/sdb
  Physical volume "/dev/sda" successfully created.
  Physical volume "/dev/sdb" successfully created.

$ pvs
 PV         VG Fmt  Attr PSize  PFree
  /dev/sda      lvm2 ---   1.00g 1.00g
  /dev/sdb      lvm2 ---   1.00g 1.00g
  /dev/vda2  cl lvm2 a--  29.00g    0 

$ pvdisplay /dev/sda
  "/dev/sda" is a new physical volume of "1.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sda
  VG Name               
  PV Size               1.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               fV1zuy-sOLb-0gOA-9B0y-2gzB-kzAQ-MKp9Qb
   
$ vgs # 볼륨 그룹 확인
  VG #PV #LV #SN Attr   VSize  VFree
  cl   1   2   0 wz--n- 29.00g    0

$ vgcreate vgtest /dev/sda /dev/sdb
  Volume group "vgtest" successfully created 

$ vgs
  VG     #PV #LV #SN Attr   VSize  VFree
  cl       1   2   0 wz--n- 29.00g    0 
  vgtest   2   0   0 wz--n-  1.99g 1.99g

$ vgdisplay vgtest
  --- Volume group ---
  VG Name               vgtest
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               1.99 GiB
  PE Size               4.00 MiB
  Total PE              510
  Alloc PE / Size       0 / 0   
  Free  PE / Size       510 / 1.99 GiB
  VG UUID               K2nsVP-0Zcy-AGjY-topt-AnkH-XvQ0-Ywoz1U

$ vgdisplay vgtest -v # 물리 볼륨, 논리 볼륨 모두 확인
  --- Volume group ---
  VG Name               vgtest
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               1.99 GiB
  PE Size               4.00 MiB
  Total PE              510
  Alloc PE / Size       0 / 0   
  Free  PE / Size       510 / 1.99 GiB
  VG UUID               K2nsVP-0Zcy-AGjY-topt-AnkH-XvQ0-Ywoz1U
   
  --- Physical volumes ---
  PV Name               /dev/sda     
  PV UUID               fV1zuy-sOLb-0gOA-9B0y-2gzB-kzAQ-MKp9Qb
  PV Status             allocatable
  Total PE / Free PE    255 / 255
   
  PV Name               /dev/sdb     
  PV UUID               hjIrgd-5MLd-f118-4Mxr-Habl-OYn3-V4FS0X
  PV Status             allocatable
  Total PE / Free PE    255 / 255

$ pvdisplay /dev/sda # 볼륨 그룹에 추가된 것을 확인
  --- Physical volume ---
  PV Name               /dev/sda
  VG Name               vgtest
  PV Size               1.00 GiB / not usable 4.00 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              255
  Free PE               255
  Allocated PE          0
  PV UUID               fV1zuy-sOLb-0gOA-9B0y-2gzB-kzAQ-MKp9Qb

$ vgremove vgtest # 볼륨 그룹 삭제
  Volume group "vgtest" successfully removed

$ vgs
  VG #PV #LV #SN Attr   VSize  VFree
  cl   1   2   0 wz--n- 29.00g    0

$ vgcreate -s 16M vgcolor /dev/sda /dev/sdb # extend가 16M인 볼륩 그룹 생성
  Volume group "vgcolor" successfully created

$ vgdisplay vgcolor
  --- Volume group ---
  VG Name               vgcolor
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               1.97 GiB
  PE Size               16.00 MiB
  Total PE              126
  Alloc PE / Size       0 / 0   
  Free  PE / Size       126 / 1.97 GiB
  VG UUID               x3hO9Q-rIoL-BcQf-qcKP-LtqF-ysu0-7d0gQG
```
-  vgdisplay
	- `PE Size               4.00 MiB` 설정하지 않으면, 기본 값이 4MB인 것을 확인

#### LVM 실습 - 논리 볼륨 
이어서 계속
```bash
$ lvs
  LV   VG Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root cl -wi-ao---- 26.00g                                                    
  swap cl -wi-ao----  3.00g

$ lvcreate -L 150M -n lvred vgcolor # extend는 16MB고 150MB 를 원하니 그보다 좀 큰 160MB가 만들어진다.
  Rounding up size to full physical extent 160.00 MiB
  Logical volume "lvred" created.

$ lvcreate -l 10 -n lvblue vgcolor
  Logical volume "lvblue" created.

$ lvcreate -L 320M vgcolor
  Logical volume "lvol0" created.

$ lvcreate -l 50%FREE vgcolor
  Logical volume "lvol1" created.
  정
$ lvs
  LV     VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao----  26.00g                                                    
  swap   cl      -wi-ao----   3.00g                                                    
  lvblue vgcolor -wi-a----- 160.00m                                                    
  lvol0  vgcolor -wi-a----- 320.00m                                                    
  lvol1  vgcolor -wi-a----- 688.00m                                                    
  lvred  vgcolor -wi-a----- 160.00m 
```
- lvcreate
	- `-L`: 용량 지정
	- `-l`: 개수 지정


#### LVM 실습 - 마운트

```bash
$ mkfs.xfs /dev/vgcolor/lvred
$ mkfs.ext4 /dev/vgcolor/lvblue
$ mkswap /dev/vgcolor/lvol1

$ mkdir /red
$ mkdir /blue

$ vi /etc/fstab
/dev/vgcolor/lvred      /red    xfs     defaults        0 2
/dev/vgcolor/lvblue     /blue   ext4    defaults        0 2
/dev/vgcolor/lvol1      swap    swap    defaults        0 0

$ mount -a # 자동 마운트
$ swapon -a # 자동 스왑

$ df # 마운트 확인
$ swapon -s # 스왑 확인
```
- /dev/{lv 그룹명}/{lv 이름}

#### LVM 실습 - 볼륨 그룹 추가
```bash
$ pvs
  PV         VG      Fmt  Attr PSize    PFree  
  /dev/sda   vgcolor lvm2 a--  1008.00m 368.00m
  /dev/sdb   vgcolor lvm2 a--  1008.00m 320.00m
  /dev/vda2  cl      lvm2 a--    29.00g      0 

$ pvcreate /dev/sdc
  Physical volume "/dev/sdc" successfully created.
$ pvs
  PV         VG      Fmt  Attr PSize    PFree  
  /dev/sda   vgcolor lvm2 a--  1008.00m 368.00m
  /dev/sdb   vgcolor lvm2 a--  1008.00m 320.00m
  /dev/sdc           lvm2 ---     1.00g   1.00g
  /dev/vda2  cl      lvm2 a--    29.00g      0 

$ vgextend vgcolor /dev/sdc
  Volume group "vgcolor" successfully extended

$ vgs
  VG      #PV #LV #SN Attr   VSize  VFree
  cl        1   2   0 wz--n- 29.00g    0 
  vgcolor   3   4   0 wz--n-  2.95g 1.66g

$ lvs
  LV     VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao----  26.00g                                                    
  swap   cl      -wi-ao----   3.00g                                                    
  lvblue vgcolor -wi-ao---- 160.00m                                                    
  lvol0  vgcolor -wi-a----- 320.00m                                                    
  lvol1  vgcolor -wi-ao---- 688.00m                                                    
  lvred  vgcolor -wi-ao---- 160.00m

$ lvextend -L 480M /dev/vgcolor/lvred
  Size of logical volume vgcolor/lvred changed from 160.00 MiB (10 extents) to 480.00 MiB (30 extents).
  Logical volume vgcolor/lvred successfully resized.

$ lvextend -l 30 /dev/vgcolor/lvblue
  Size of logical volume vgcolor/lvblue changed from 160.00 MiB (10 extents) to 480.00 MiB (30 extents).
  Logical volume vgcolor/lvblue successfully resized.

$ lvs
  LV     VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao----  26.00g                                                    
  swap   cl      -wi-ao----   3.00g                                                    
  lvblue vgcolor -wi-ao---- 480.00m                                                    
  lvol0  vgcolor -wi-a----- 320.00m                                                    
  lvol1  vgcolor -wi-ao---- 688.00m                                                    
  lvred  vgcolor -wi-ao---- 480.00m

$ df -h # lv의 용량을 늘려도 파일시스템의 용량이 이에 맞게 자동으로 증가하지는 않는다.
Filesystem                  Size  Used Avail Use% Mounted on
/dev/mapper/cl-root          26G  4.5G   22G  18% /
devtmpfs                    1.5G     0  1.5G   0% /dev
tmpfs                       1.5G  140K  1.5G   1% /dev/shm
tmpfs                       1.5G  8.9M  1.5G   1% /run
tmpfs                       1.5G     0  1.5G   0% /sys/fs/cgroup
/dev/vda1                  1014M  157M  858M  16% /boot
tmpfs                       304M  4.0K  304M   1% /run/user/42
tmpfs                       304M   28K  304M   1% /run/user/0
/dev/mapper/vgcolor-lvred   157M  8.2M  149M   6% /red
/dev/mapper/vgcolor-lvblue  151M  1.6M  139M   2% /blue

```
- **lvextend로  lv의 용량을 증가시켜도 파일 시스템의 용량이 자동으로 이에 맞게 증가하지 않는다.**
- lvextend
	- `-L`: 증가 후 최종 용량, 증가분이 아님

##### 파일 시스템 증가
```bash
$ xfs_growfs /dev/vgcolor/lvred
meta-data=/dev/mapper/vgcolor-lvred isize=512    agcount=4, agsize=10240 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0 spinodes=0
data     =                       bsize=4096   blocks=40960, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=855, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 40960 to 122880

$ resize2fs /dev/vgcolor/lvblue
resize2fs 1.42.9 (28-Dec-2013)
Filesystem at /dev/vgcolor/lvblue is mounted on /blue; on-line resizing required
old_desc_blocks = 2, new_desc_blocks = 4
The filesystem on /dev/vgcolor/lvblue is now 491520 blocks long.

$ df -h
Filesystem                  Size  Used Avail Use% Mounted on
/dev/mapper/cl-root          26G  4.5G   22G  18% /
devtmpfs                    1.5G     0  1.5G   0% /dev
tmpfs                       1.5G  140K  1.5G   1% /dev/shm
tmpfs                       1.5G  8.9M  1.5G   1% /run
tmpfs                       1.5G     0  1.5G   0% /sys/fs/cgroup
/dev/vda1                  1014M  157M  858M  16% /boot
tmpfs                       304M  4.0K  304M   1% /run/user/42
tmpfs                       304M   28K  304M   1% /run/user/0
/dev/mapper/vgcolor-lvred   477M  8.5M  469M   2% /red
/dev/mapper/vgcolor-lvblue  461M  2.3M  435M   1% /blue
```
- `xfs_growfs`: xfs 파일 시스템에서 사용
- `resize2fs` ext4 파일 시스템에서 사용 

##### 파일 시스템 축소
```bash
$ umount /blue # 축소 시키기 전에 마운트 해제 필요

$ resize2fs /dev/vgcolor/lvblue 320M # 줄이는 건 바로 가능하지 않고 유실 가능성을 먼저 체크해야한다
resize2fs 1.42.9 (28-Dec-2013)
Please run 'e2fsck -f /dev/vgcolor/lvblue' first.

$ e2fsck -f /dev/vgcolor/lvblue
e2fsck 1.42.9 (28-Dec-2013)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
/dev/vgcolor/lvblue: 11/122880 files (0.0% non-contiguous), 21922/491520 blocks

resize2fs /dev/vgcolor/lvblue 320M
resize2fs 1.42.9 (28-Dec-2013)
Resizing the filesystem on /dev/vgcolor/lvblue to 327680 (1k) blocks.
The filesystem on /dev/vgcolor/lvblue is now 327680 blocks long.
```
- xfs 파일 시스템은 아직 줄이는 것이 존재하지 않다.

##### lv 축소
파일 시스템을 줄였으니 논리 볼륨의 사이즈도 맞춰 줄여준다.
``` bash
lvs
  LV     VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao----  26.00g                                                    
  swap   cl      -wi-ao----   3.00g                                                    
  lvblue vgcolor -wi-a----- 480.00m                                                    
  lvol0  vgcolor -wi-a----- 320.00m                                                    
  lvol1  vgcolor -wi-ao---- 688.00m                                                    
  lvred  vgcolor -wi-ao---- 480.00m 
  
$ lvreduce -L 320M /dev/vgcolor/lvblue
  WARNING: Reducing active logical volume to 320.00 MiB.
  THIS MAY DESTROY YOUR DATA (filesystem etc.)
Do you really want to reduce vgcolor/lvblue? [y/n]: y
  Size of logical volume vgcolor/lvblue changed from 480.00 MiB (30 extents) to 320.00 MiB (20 extents).
  Logical volume vgcolor/lvblue successfully resized.

$ lvs
  LV     VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao----  26.00g                                                    
  swap   cl      -wi-ao----   3.00g                                                    
  lvblue vgcolor -wi-a----- 320.00m                                                    
  lvol0  vgcolor -wi-a----- 320.00m                                                    
  lvol1  vgcolor -wi-ao---- 688.00m                                                    
  lvred  vgcolor -wi-ao---- 480.00m

$ mount -a
$ df -h
Filesystem                  Size  Used Avail Use% Mounted on
/dev/mapper/cl-root          26G  4.5G   22G  18% /
devtmpfs                    1.5G     0  1.5G   0% /dev
tmpfs                       1.5G  140K  1.5G   1% /dev/shm
tmpfs                       1.5G  8.9M  1.5G   1% /run
tmpfs                       1.5G     0  1.5G   0% /sys/fs/cgroup
/dev/vda1                  1014M  157M  858M  16% /boot
tmpfs                       304M  4.0K  304M   1% /run/user/42
tmpfs                       304M   28K  304M   1% /run/user/0
/dev/mapper/vgcolor-lvred   477M  8.5M  469M   2% /red
/dev/mapper/vgcolor-lvblue  306M  2.1M  287M   1% /blue
```
#### LVM 실습 - 볼륨 그룹 삭제
1. v
```bash
# 스왑된 파일시스템 off
$ swapoff /dev/vgcolor/lvol1
$ swapon -s

# 마운트된 파일시스템 해제
$ umount /red
$ umount /blue

# 논리 볼륨 제거
$ lvremove /dev/vgcolor/lvol0
Do you really want to remove active logical volume vgcolor/lvol0? [y/n]: y
  Logical volume "lvol0" successfully removed

$ lvs
  LV     VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao----  26.00g                                                    
  swap   cl      -wi-ao----   3.00g                                                    
  lvblue vgcolor -wi-a----- 320.00m                                                    
  lvol1  vgcolor -wi-a----- 688.00m                                                    
  lvred  vgcolor -wi-a----- 480.00m

# 논리 그룹 제거 (논리 볼륨 한번에 제거하는 방법)
$ vgremove vgcolor
Do you really want to remove volume group "vgcolor" containing 3 logical volumes? [y/n]: y
Do you really want to remove active logical volume vgcolor/lvred? [y/n]: y
  Logical volume "lvred" successfully removed
Do you really want to remove active logical volume vgcolor/lvblue? [y/n]: y
  Logical volume "lvblue" successfully removed
Do you really want to remove active logical volume vgcolor/lvol1? [y/n]: y
  Logical volume "lvol1" successfully removed
  Volume group "vgcolor" successfully removed

$ lvs
lvs
  LV   VG Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root cl -wi-ao---- 26.00g                                                    
  swap cl -wi-ao----  3.00g  

$ vgs
  VG #PV #LV #SN Attr   VSize  VFree
  cl   1   2   0 wz--n- 29.00g    0


# 물리 볼륨 제거
$ pvs
  PV         VG Fmt  Attr PSize  PFree
  /dev/sda      lvm2 ---   1.00g 1.00g
  /dev/sdb      lvm2 ---   1.00g 1.00g
  /dev/sdc      lvm2 ---   1.00g 1.00g
  /dev/vda2  cl lvm2 a--  29.00g    0 

$ pvremove /dev/sdc # 디스크를 물리적으로 제거하기 전에 시스템에서 삭제(물리적 제거란 현재 VMM 시스템에서 하드웨어 제거하는 행위)
  Labels on physical volume "/dev/sdc" successfully wiped.
$ pvs
  PV         VG Fmt  Attr PSize  PFree
  /dev/sda      lvm2 ---   1.00g 1.00g
  /dev/sdb      lvm2 ---   1.00g 1.00g
  /dev/vda2  cl lvm2 a--  29.00g    0 
```


#### LVM 활성화
SSHD: SSD + HDD,  OS가 설치될 일부 영역만 SSD로 구성되고 나머지는 HDD 구성됨

lv에서도 이러한 기능이 가능

|Data [캐시]|
|--|
|Data LV [캐시]|
|VG|
|HDD + HDD + SSD [캐시]|
- write-back
	- 시스템 다운 시 데이터 유실 가능성
- write-through(default)


```bash
$ pvcreate /dev/sdc # 위에서 삭제한 내용 다시 생성
  Physical volume "/dev/sdc" successfully created.

$ pvs
  PV         VG Fmt  Attr PSize  PFree
  /dev/sda      lvm2 ---   1.00g 1.00g
  /dev/sdb      lvm2 ---   1.00g 1.00g
  /dev/sdc      lvm2 ---   1.00g 1.00g
  /dev/vda2  cl lvm2 a--  29.00g    0 

$ vgcreate vgcloud /dev/sda /dev/sdb /dev/sdc
  Volume group "vgcloud" successfully created

$ pvs -o +tags # 태그 확인
  PV         VG      Fmt  Attr PSize    PFree    PV Tags
  /dev/sda   vgcloud lvm2 a--  1020.00m 1020.00m        
  /dev/sdb   vgcloud lvm2 a--  1020.00m 1020.00m        
  /dev/sdc   vgcloud lvm2 a--  1020.00m 1020.00m        
  /dev/vda2  cl      lvm2 a--    29.00g       0  

# 태그 생성
$ pvchange --addtag slowhdd /dev/sda /dev/sdb
  Physical volume "/dev/sda" changed
  Physical volume "/dev/sdb" changed
  2 physical volumes changed / 0 physical volumes not changed
$ pvchange --addtag ssd /dev/sdc
  Physical volume "/dev/sdc" changed
  1 physical volume changed / 0 physical volumes not changed
  
$ pvs -o +tags
  PV         VG      Fmt  Attr PSize    PFree    PV Tags
  /dev/sda   vgcloud lvm2 a--  1020.00m 1020.00m slowhdd
  /dev/sdb   vgcloud lvm2 a--  1020.00m 1020.00m slowhdd
  /dev/sdc   vgcloud lvm2 a--  1020.00m 1020.00m ssd    
  /dev/vda2  cl      lvm2 a--    29.00g       0    


$ lvcreate -l 100%FREE -n lvdata vgcloud @slowhdd # 데이터 영역
WARNING: xfs signature detected on /dev/vgcloud/lvdata at offset 0. Wipe it? [y/n]: y
  Wiping xfs signature on /dev/vgcloud/lvdata.
  Logical volume "lvdata" created.

$ lvcreate --type cache-pool -l 100%FREE -n 역lvcache vgcloud @ssd # 캐시 영역 
  Using default stripesize 64.00 KiB.
  Logical volume "lvcache" created.

$ lvs # 현 시점에서 분리된 것으로 보임
  LV      VG      Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root    cl      -wi-ao----   26.00g                                                    
  swap    cl      -wi-ao----    3.00g                                                    
  lvcache vgcloud Cwi---C--- 1004.00m                                                    
  lvdata  vgcloud -wi-a-----    1.99g 

$ vgdisplay -v vgcloud # cache는 단독으로 사용할 수 없기 때문에 not available
# lvdata
#  LV Status              availabl
# lvcache
#  LV Status              NOT available


$ lvconvert --type cache --cachepool vgcloud/lvcache vgcloud/lvdata # 두개를 합쳐 하나로 사용
Do you want wipe existing metadata of cache pool volume vgcloud/lvcache? [y/n]: y
  Logical volume vgcloud/lvdata is now cached.

$ lvs # lv가 하나로 합쳐진 것을 확인
  LV     VG      Attr       LSize  Pool      Origin         Data%  Meta%  Move Log Cpy%Sync Convert
  root   cl      -wi-ao---- 26.00g                                                                 
  swap   cl      -wi-ao----  3.00g                                                                 
  lvdata vgcloud Cwi-a-C---  1.99g [lvcache] [lvdata_corig] 0.00   1.86            0.00       

# 읽기 성능 향상, SSD의 캐시를 이용하기 때문에
$ mkfs.xfs /dev/vgcloud/lvdata
$ mkdir /test에
$ mount /dev/vgcloud/lvdata /test
$ cp /etc/*.conf /test

$ lvs -o lv_name,cache_mode,cache_read,hits,cache_write_hits
```

## NAS

### NFS
#### NFSv4
`2049` 포트 단일화

제약 조건
- kernel 2.6.33+
- nfs-utils 1.2.2+

#### NFS Client
**host 컴퓨터에서 진행**

`server1`은 day01에 /etc/hosts 에 설정한 서버
`59.29.224.177 server1.example.com server1`

```bash
$ showmount -e server1
Export list for server1:
/export/courserepos *
/export/autoyast    *
/export/home        *
/export/tmp         *
/export/netinstall  *
/export/kickstart   *

$ mkdir /aaa
$ mount -t nfs server1:/export/home /aaa

$ df
...
server1:/export/home 487108608 21967872 465140736   5% /aaa
$ ls /aaa
...

$ mount | grep /aaa
server1:/export/home on /aaa type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=59.29.224.192,local_lock=none,addr=59.29.224.177)

$ umount /aaa

$ mount -t nfs -o vers=3 server1:/export/home /aaa
$ mount | grep /aaa
server1:/export/home on /aaa type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=59.29.224.177,mountvers=3,mountport=20048,mountproto=udp,local_lock=none,addr=59.29.224.177)
```

### NFS Server
1. host, VM에 같은 세팅
	- setenforce
	- systemctl
2. nfs-server (VM)
3. client(host)
	- mount


#### host
```bash
# host
$ setenforce 0
$ systemctl stop firewalld
```

####  VM
nfs-server
192.168.122.149
```bash
# VM
$ setenforce 0
$ systemctl stop firewalld

$ mkdir /nfsdata
$ touch /nfsdata/sample
$ chmod 1777 /nfsdata

$ vi /etc/exports # 이 정보를 기반으로 nfs-server 구동
/nfsdata        192.168.122.0/24(rw,sync)  59.29.224.5(ro)

$ systemctl status nfs-server
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; disabled; vendor preset: disabled)
   Active: inactive (dead)

$ systemctl start nfs-server

$ systemctl status nfs-server
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; disabled; vendor preset: disabled)
   Active: active (exited) since Wed 2020-02-26 14:31:25 KST; 14s ago
  Process: 9060 ExecStart=/usr/sbin/rpc.nfsd $RPCNFSDARGS (code=exited, status=0/SUCCESS)
  Process: 9059 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS)
 Main PID: 9060 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-server.service

Feb 26 14:31:25 localhost.localdomain systemd[1]: Starting NFS server and services...
Feb 26 14:31:25 localhost.localdomain systemd[1]: Started NFS server and services.
```

```bash
# host
$ showmount -e 192.168.122.149
Export list for 192.168.122.149:
/nfsdata 192.168.122.0/24,59.29.224.5

$ mount -t nfs 192.168.122.149:/nfsdata /mnt

$ ll /mnt
-rw-r--r--. 1 root root 0 Feb 26 14:27 sample

$ whoami
root
$ touch /mnt/1.txt
$ ll /mnt # 새로 만든 파일의 소유권은 root가 아니다.
total 0
-rw-r--r--. 1 nfsnobody nfsnobody 0 Feb 26 14:35 1.txt
-rw-r--r--. 1 root      root      0 Feb 26 14:27 sample
```

```bash
# VM
$ vi /etc/exports # 설정 수정
nfsdata        192.168.122.0/24(rw,sync,no_root_squash)  59.29.224.5(ro)

$ exportfs -vra
exporting 59.29.224.5:/nfsdata
exporting 192.168.122.0/24:/nfsdata
```
- 설정을 바꾼 뒤 `nfs-server` 서버를 다시 시작하지 않고 `exportfs -vra`를 이용하여 설정을 다시 불러온다.

```bash
# host
$ touch /mnt/2.txt
$ ll /mnt
total 0
-rw-r--r--. 1 nfsnobody nfsnobody 0 Feb 26 14:35 1.txt
-rw-r--r--. 1 root      root      0 Feb 26 14:38 2.txt
-rw-r--r--. 1 root      root      0 Feb 26 14:27 sample
```
- `2.txt`는 `root` 소유로 생성된다.


#### AutoFS
> 클라이언트의 설정으로
> 사용할 때 moun
> 사용하지 않으면 umount

1. 이전 실습 모두 umount


```bash
$ df
...
server1:/export/home     487108608 21967872 465140736   5% /aaa
192.168.122.149:/nfsdata  27246080  4650496  22595584  18% /mnt


$ umount /aaa
$ umount /mnt

$ cat -n /etc/auto.master
...
/misc	/etc/auto.misc

$ cat -n /etc/auto.misc
...
cd		-fstype=iso9660,ro,nosuid,nodev	:/dev/cdrom
```

##### /etc/auto.master
|접근 디렉토리| 설정 파일|
|--|--|
|/misc|/etc/auto.misc|
- `/misc`: 클라이언트 PC에 접근할 디렉토리
- `/etc/auto.misc`: 해당 설정 파일로 자동으로 마운트

##### /etc/auto.misc

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTQyNzczOTAxLC03NzExMzI4OTcsMTI2ND
ExODczMiw4OTQyNzExMTQsLTI5ODEwMjQ0Niw1NDkxOTQ0NDMs
LTY1OTI5NzAxMSwxNjczODY3MjEwLC0xMTA2OTgyODU5LC03Mj
Y0NTUzNTIsLTcyMzgyNDUwOCwxNjQ5ODU3ODAxLDc1ODM2MzI1
MCwxMTUwMjc0NjIyLC0xNDA2NjUwMDI0LDU0NDA0ODUzMSwyMD
kyMDg4Mzc1LC0yMDg4NzQ2NjEyXX0=
-->